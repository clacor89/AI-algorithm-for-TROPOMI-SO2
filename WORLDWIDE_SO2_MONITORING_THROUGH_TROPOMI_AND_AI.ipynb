{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clacor89/AI-algorithm-for-TROPOMI-SO2/blob/main/WORLDWIDE_SO2_MONITORING_THROUGH_TROPOMI_AND_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RewJmm13d13C"
      },
      "source": [
        "*Code by Claudia Corradino-INGV Catania.*\\\n",
        "*Reference: Corradino, C., Jouve, P., La Spina, A., & Del Negro, C. (2024). Monitoring Earth's atmosphere with Sentinel-5 TROPOMI and Artificial Intelligence: Quantifying volcanic SO2 emissions. Remote Sensing of Environment, 315, 114463.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions**:\n",
        "1) You will need to create a google and google earth engine account if not created yet. Click consense for use of Google services. Type your Google Earth Engine (GEE) project in the \"gee_name\" variable. YOU SHOULD BE LOGGED INTO COLAB WITH THE SAME GOOGLE ACCOUNT YOU SET FOR DRIVE AND GEE\n",
        "2) Set the starting and ending date\n",
        "3) Select one of the options to choose volcano/volcanoes by setting 'volcano_selection_method' to analyze among (1) from dropdown list,(2) by typing the name of the volcano/ list of volcanoesor (3) by selecting all volcanoes in a Region\n",
        "\n",
        "The output files will be in the Drive folder: MyDrive/TROPOMI_RF:\n",
        "\n",
        "- .png of the detected mask  (uncomment if you want to plot and save)\n",
        "- .png  (uncomment if you want to plot and save), .geotiff and .nc of the detected so2\n",
        "- Excel file containing so2 mass(kton) of the analyzed scenes, total of the scene and specific for the investigated volcano  "
      ],
      "metadata": {
        "id": "ZskvcEGHSqJB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvFewfOQXzFE"
      },
      "source": [
        "\n",
        "# *Libraries*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "crvLhZEvW2eE"
      },
      "outputs": [],
      "source": [
        "!pip install geemap\n",
        "import geemap\n",
        "from datetime import datetime\n",
        "import openpyxl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install xarray\n",
        "!pip install netCDF4\n",
        "!pip install numpy\n",
        "import netCDF4\n",
        "import numpy as np\n",
        "import xarray\n",
        "from osgeo import gdal\n",
        "import os\n",
        "from skimage.measure import label, regionprops\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "!pip install geemap\n",
        "import geemap\n",
        "!pip install --force-reinstall gdal==3.6.4\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import matplotlib.image\n",
        "from matplotlib import cm\n",
        "import cv2\n",
        "import rasterio as rs\n",
        "from matplotlib import pyplot\n",
        "from google.colab import drive\n",
        "import ipywidgets as widgets\n",
        "import logging\n",
        "logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Input*"
      ],
      "metadata": {
        "id": "oNQovzE0lKg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GOOGLE AND GOOGLE EARTH ENGINE ACCOUNT { run: \"auto\", form-width: \"50%\" }\n",
        "\n",
        "#insert your GOOGLE EARTH ENGINE project name\n",
        "gee_project_name = 'victor-corradinoclaudia' #@param {type: \"string\"}\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "path='/content/drive/MyDrive/TROPOMI_RF'#@param {type: \"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qOWzYw7CbEMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Insert dates of interest and volcano selection features { run: \"auto\", form-width: \"50%\" }\n",
        "\n",
        "start_day = '2025-12-01' #@param {type: \"date\"}\n",
        "end_day = '2025-12-03' #@param {type: \"date\"}\n",
        "\n",
        "#insert volcano name/ names ['Shishaldin', 'Buldir']\n",
        "volcano_selection_method=2#@param {type: \"raw\"}\n",
        "volcano_name=['Etna', 'Stromboli'] #@param {type: \"raw\"}\n",
        "volcano_region=' ' #@param {type: \"string\"}\n",
        "radius= 600000 #@param {type: \"raw\"}\n"
      ],
      "metadata": {
        "id": "ZONJ-BXG88DL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PqWs3IQM5f6"
      },
      "source": [
        "#*Functions Definition*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFFF6JSEu2vs"
      },
      "outputs": [],
      "source": [
        "def featurecollection_to_dataframe(fc, batch_size=5000):\n",
        "    \"\"\"Converti FeatureCollection in pandas DataFrame.\n",
        "    Se piccola, usa getInfo().\n",
        "    Se grande, esporta su Drive e leggi CSV.\n",
        "    \"\"\"\n",
        "    # Proviamo con getInfo() prima\n",
        "    try:\n",
        "        features = fc.getInfo()['features']\n",
        "        data = [f['properties'] for f in features]\n",
        "        df = pd.DataFrame(data)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        # Esporta su Drive\n",
        "        task = ee.batch.Export.table.toDrive(\n",
        "            collection=fc,\n",
        "            description='export_temp',\n",
        "            fileFormat='CSV'\n",
        "        )\n",
        "        task.start()\n",
        "\n",
        "        # Attendi che il task finisca\n",
        "        while task.active():\n",
        "            print(\"In corso...\")\n",
        "            time.sleep(5)\n",
        "\n",
        "        return None\n",
        "\n",
        "def CountBands(image):\n",
        "  return image.set('count',image.bandNames().length())\n",
        "\n",
        "def InputPreparation(image):\n",
        "  #selecting needed GLCM bands form inital 18 ones\n",
        "  bands_glcm = ['glcm_contrast','glcm_corr','glcm_var','glcm_savg']\n",
        "  #bands used for Classification + select(add) bands from glcm\n",
        "  bands = ee.List(['SO2_column_number_density','so2_15km','sd','entropy']).cat(ee.List(bands_glcm))\n",
        "\n",
        "  # Kernel for Stdv and entropy\n",
        "  bigKernel = ee.Kernel.square(\n",
        "      radius = 4,\n",
        "      units = 'pixels')\n",
        "\n",
        "  #Kernel for Convolution\n",
        "  l3 = [-0.4444,-0.4444, -0.1111, -0.4444,-0.4444]\n",
        "\n",
        "  #Center of Kernel is zero\n",
        "  c3 = [-0.1111, -0.1111,3.222, -0.1111,-0.1111]\n",
        "\n",
        "  #Assemble a list of lists, the 9x9 kernel weights as a 2D matrix\n",
        "  lists3 = [l3,l3,c3,l3,l3]\n",
        "  kernel3 = ee.Kernel.fixed(5,5,lists3)\n",
        "\n",
        "  #Kernel for focalMin/focalMax\n",
        "  kernel = ee.Kernel.circle(radius = 2)\n",
        "\n",
        "  ###First training image : '2022-07-27', '2022-07-28'\n",
        "    #median to take average pixel value from remaning images of collection, multiply to convert into dobson unit\n",
        "  SO2_at_1km = image.select('SO2_column_number_density').multiply(10000/4.4615)\\\n",
        "\n",
        "  SO2_at_15km = image.select('SO2_column_number_density_15km').multiply(10000/4.4615)\\\n",
        "\n",
        "  #Compute standard derivation (Texture)\n",
        "  sd = SO2_at_1km.reduceNeighborhood(\n",
        "   reducer= ee.Reducer.stdDev(),\n",
        "   kernel= bigKernel\n",
        "   ).reproject('EPSG:4326',None,3500)\n",
        "\n",
        "  #Compute Entropy (Texture)\n",
        "  intso2=SO2_at_1km.multiply(10000).int()\n",
        "  entropy=intso2.entropy(bigKernel).reproject('EPSG:4326',None,3500)\n",
        "\n",
        "  #Compute Graylevel Co-occurence matrix\n",
        "  glcm = intso2.rename('glcm').glcmTexture(size= 1).reproject('EPSG:4326',None,3500).select(bands_glcm)#vor select!\n",
        "  sd1 = sd.unmask(-100);\n",
        "  entropy1 = entropy.unmask(-100);\n",
        "  glcm1 = glcm.unmask(-100);\n",
        "\n",
        "  input = SO2_at_1km.unmask(-100).rename('SO2_column_number_density').addBands(SO2_at_15km.rename('so2_15km')).addBands(sd1.rename('sd')).addBands(entropy1.rename('entropy')).addBands(glcm1)\n",
        "\n",
        "  input = input.set('name',image.get('name')).set('Date', image.get('Date')).set('system:time_start', image.get('system:time_start'))\n",
        "\n",
        "  date=image.get('system:time_start')\n",
        "  start = ee.Date(date)\n",
        "  end = start.advance(1, 'day');\n",
        "  dataset =ee.ImageCollection('COPERNICUS/S5P/NRTI/L3_CLOUD').filterDate(start,end).filterBounds(study_zone).select('cloud_top_height').max().unmask(-100)\n",
        "  return image.select('SO2_column_number_density').rename('in').addBands(input).addBands(dataset.rename('altitude')).addBands(SO2_at_1km.rename('input'))\n",
        "\n",
        "def fromgeecollectiontonumpy(ee_object,out_dir,scale=None, crs=None, region=None, file_per_band=False\n",
        "):\n",
        "    \"\"\"Exports an ImageCollection as GeoTIFFs.\n",
        "\n",
        "    Args:\n",
        "        ee_object (object): The ee.Image to download.\n",
        "        out_dir (str): The output directory for the exported images.\n",
        "        scale (float, optional): A default scale to use for any bands that do not specify one; ignored if crs and crs_transform is specified. Defaults to None.\n",
        "        crs (str, optional): A default CRS string to use for any bands that do not explicitly specify one. Defaults to None.\n",
        "        region (object, optional): A polygon specifying a region to download; ignored if crs and crs_transform is specified. Defaults to None.\n",
        "        file_per_band (bool, optional): Whether to produce a different GeoTIFF per band. Defaults to False.\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(ee_object, ee.ImageCollection):\n",
        "        print(\"The ee_object must be an ee.ImageCollection.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "\n",
        "    try:\n",
        "\n",
        "        count = int(ee_object.size().getInfo())\n",
        "        print(f\"Total number of images: {count}\\n\")\n",
        "        print(f\"Processing:\")\n",
        "        img_stack = []\n",
        "        for i in range(0, count):\n",
        "            image = ee.Image(ee_object.toList(count).get(i))\n",
        "            image_array = geemap.ee_to_numpy(image.select('classification','SO2corrected'), region) # Client side\n",
        "            img_stack.append(image_array)\n",
        "\n",
        "        # Normalize array shapes and get min shape across all arrays (height, width)\n",
        "        min_h = min(arr.shape[0] for arr in img_stack)\n",
        "        min_w = min(arr.shape[1] for arr in img_stack)\n",
        "\n",
        "        # Crop or pad each array to (min_h, min_w)\n",
        "        normalized_stack = []\n",
        "        for arr in img_stack:\n",
        "            h, w = arr.shape[:2]\n",
        "            # Crop larger arrays\n",
        "            arr_cropped = arr[:min_h, :min_w, ...]\n",
        "            # Pad smaller ones if needed (not strictly necessary here)\n",
        "            if h < min_h or w < min_w:\n",
        "                pad_h = min_h - h\n",
        "                pad_w = min_w - w\n",
        "                arr_cropped = np.pad(arr_cropped,((0, pad_h), (0, pad_w), (0, 0)),mode='constant',constant_values=np.nan,)\n",
        "            normalized_stack.append(arr_cropped)\n",
        "        return normalized_stack\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def apply_classifier(image3,ee_classifier):\n",
        "  alt=image3.select('altitude')\n",
        "  image2=image3.select(['SO2_column_number_density','so2_15km','sd','entropy','glcm_contrast','glcm_corr','glcm_var','glcm_savg']);\n",
        "  image=image2.subtract(means).divide(std)\n",
        "  so2_plume_15km=image3.select('so2_15km');\n",
        "  so2_plume=image3.select('SO2_column_number_density')\n",
        "  so2_median_density = so2_plume.unmask(-100);\n",
        "  so2_classify_mask = image.classify(ee_classifier)\n",
        "  so2_classify = so2_median_density.updateMask(so2_classify_mask)\n",
        "\n",
        "  # Post-treatment : Apply a closing (dilatation = focalMax then erosion = focalMin)\n",
        "  # Define a circle kernel of 1 pixel radius (adjust if needed, try 1.5)\n",
        "  kernel = ee.Kernel.circle(\n",
        "      radius = 1,\n",
        "      units = 'pixels')\n",
        "  connectivity=50;\n",
        "  #Closing is better than opening to fullfil the plume and then apply PixelCount\n",
        "  #2 iterations of dilatation and 3 of erosion seems to give the best results (adjust if needed)\n",
        "  closed = so2_classify_mask.focalMax(\n",
        "   kernel= kernel,\n",
        "   iterations= 2).focalMin(\n",
        "   kernel= kernel,\n",
        "   iterations= 3).reproject('EPSG:4326',None,3500);\n",
        "  so2_closed = so2_median_density.updateMask(closed)\n",
        "\n",
        "  # Connectivity condition : to delete isolated pixels and keep only pixels part of the plume\n",
        "  # ConnectedPixelCount returns a connection score of the SO2-positive pixels\n",
        "  connexions = closed.selfMask().connectedPixelCount(ee.Number(connectivity).add(1)).reproject('EPSG:4326',None,3500);\n",
        "  proxy = 0\n",
        "  connexions = connexions.unmask(proxy)\n",
        "\n",
        "  # Only keep the pixels that have a connectivity high enough\n",
        "  # Score connectivity of 80 seems to be a good value, adjust if needed\n",
        "  groupesPixels = connexions.gt(connectivity);\n",
        "  positive_pixel = groupesPixels.reproject('EPSG:4326',None,3500);\n",
        "\n",
        "  # Update the mask to the so2 map\n",
        "  so2_plume = so2_median_density.updateMask(positive_pixel)\n",
        "  # Update the mask to the so2 map\n",
        "  so2_plume_15km = image.select('so2_15km').updateMask(positive_pixel)\n",
        "\n",
        "  m2=positive_pixel.multiply(alt)\n",
        "  m3=m2.updateMask(m2.gt(-100));\n",
        "  mmean=ee.Image(ee.Number(m2.reduceRegion(reducer= ee.Reducer.max(),geometry= study_zone,scale= res,maxPixels= 1e13).values().get(0)));\n",
        "  altcorreted=(positive_pixel.multiply(alt)).selfMask();\n",
        "  m=m2.where(m2.eq(-100),mmean)\n",
        "  flag=(so2_plume_15km.subtract(so2_plume)).multiply(m.subtract(10000)).divide(9000).add(so2_plume_15km);\n",
        "  so2_corrected=so2_plume_15km.where(so2_plume.gte(0),flag.where(m.gt(10000),so2_plume_15km)).multiply(4.4615/10000).multiply(ee.Image.pixelArea()).multiply(0.000001*64*0.001)#\n",
        "  st2=ee.Geometry.Polygon([[[158.6128787089055, 55.34598773699004],[158.6128787089055, 53.8505541387213],[164.4246462870305, 53.8505541387213],[164.4246462870305, 55.34598773699004]]])#, null, false);\n",
        "  so2_original=so2_plume_15km#\n",
        "\n",
        "  # Convert VCD from DU to mol/m2 (1DU=4.4615*10^-4mol/m2)\n",
        "  kton=so2_corrected.reduceRegion(\n",
        "          reducer= ee.Reducer.sum(),\n",
        "          geometry= so2_corrected.geometry(),\n",
        "          scale= res,\n",
        "          maxPixels= 1e13\n",
        "  ).values().get(0);\n",
        "\n",
        "  output = image3.select('in').addBands(so2_plume).addBands(so2_plume_15km).addBands(positive_pixel.rename('classification')).addBands(m.selfMask().rename('masked_alt')).addBands(so2_original.rename('input')).addBands(image3.select('altitude')).addBands(so2_corrected.rename('SO2corrected')).set('name',image3.get('name')).set('kton',kton).set('Date', image3.get('Date')).set('system:time_start', image3.get('system:time_start'))\n",
        "\n",
        "      #m.rename('masked_alt')).addBands(so2_corrected.rename('SO2corrected')).addBands(alt.rename('altitude'))\n",
        "      #.set('kton',kton).set('Total_mass_corrected',total_mass111).set('Total_mass_1km',total_mass1).set('Total_mass_15km',total_mass15)//.addBands(ee.Image.constant(total_mass).rename('Total_mass'))\n",
        "\n",
        "  return output\n",
        "  #return classified.set('system:time_start', image.get('system:time_start'))\n",
        "\n",
        "# Function to generate a list of the dates of the time series\n",
        "def dayOffset(day):\n",
        "  return startDate.advance(day, 'day')\n",
        "\n",
        "def generateDateList(startDate, endDate):\n",
        "  #Calculate the date range in milliseconds\n",
        "  timeDiff = endDate.millis().subtract(startDate.millis());\n",
        "  #Convert the difference in days\n",
        "  daysDiff = timeDiff.divide(1000 * 60 * 60 * 24);\n",
        "  dateList = ee.List.sequence(0, daysDiff.toInt());\n",
        "  #Calculate the date from the startDate\n",
        "  dateRange = dateList.map(dayOffset);\n",
        "  return dateRange;\n",
        "\n",
        "def One_image_per_dayfun(date):\n",
        "    start = ee.Date(date)\n",
        "    end = start.advance(1, 'day')\n",
        "\n",
        "    filtered = raw.filterDate(start, end).filterBounds(study_zone)\n",
        "    first_image = filtered.first()\n",
        "\n",
        "    # Se non c'Ã¨ immagine, ritorna un'immagine vuota (nulla)\n",
        "    return ee.Algorithms.If(\n",
        "        first_image,\n",
        "        filtered \\\n",
        "            .select(['SO2_column_number_density', 'SO2_column_number_density_15km']) \\\n",
        "            .median() \\\n",
        "            .clip(study_zone) \\\n",
        "            .set('system:time_start', first_image.get('system:time_start')) \\\n",
        "            .set('name', ee.Date(first_image.get('system:time_start')).format('dd-MM-YYYY'))\\\n",
        "            .set('Date', ee.Date(first_image.get('system:time_start')).format('dd/MM/YYYY')),\n",
        "        None\n",
        "    )\n",
        "def add_time_dim(xda):\n",
        "     xda = xda.expand_dims(time = [datetime(2024,12,1)])\n",
        "     return xda\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8bZ5nP4X8kI"
      },
      "source": [
        "# *GEE authentication*\n",
        "Accessing Google Earth Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX2Qo4j2XaxX",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "# Trigger the authentication flow.\n",
        "ee.Authenticate()\n",
        "# Initialize the library.\n",
        "ee.Initialize(project=gee_project_name)\n",
        "\n",
        "#Load the trained classifier and normalization coefficients*\n",
        "assetId = 'projects/victor-corradinoclaudia/assets/RF_Tropomi'\n",
        "std = ee.Image('projects/victor-corradinoclaudia/assets/means_tropomi');\n",
        "means = ee.Image('projects/victor-corradinoclaudia/assets/std_tropomi');\n",
        "fc = ee.FeatureCollection('projects/victor-corradinoclaudia/assets/VolcanoList')\n",
        "df = featurecollection_to_dataframe(fc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Select one option to select volcanoes based on the variable \"volcano_selection_method\":*\n",
        "\n",
        "1.   from dropdown list\n",
        "2.   by typing the name of the volcano/ list of volcanoes\n",
        "3.   by selecting all volcanoes in a Region\n",
        "\n"
      ],
      "metadata": {
        "id": "7e1s0xrf7ztR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "match volcano_selection_method:\n",
        "    case 1:\n",
        "        names = df['Volcano_Name'].dropna().tolist()\n",
        "        # Crea il menu a tendina\n",
        "        dropdown = widgets.Dropdown(\n",
        "            options=sorted(names),\n",
        "            description='Name:',\n",
        "        )\n",
        "\n",
        "        # Bottone\n",
        "        button = widgets.Button(description=\"Submit\")\n",
        "\n",
        "        # Output\n",
        "        output = widgets.Output()\n",
        "\n",
        "        # Callback\n",
        "        def on_click(b):\n",
        "            with output:\n",
        "                output.clear_output()\n",
        "                print(f\"Selected volcano: {dropdown.value}\")\n",
        "        button.on_click(on_click)\n",
        "        display(dropdown, button, output)\n",
        "    case 2:\n",
        "        names = df['Volcano_Name'].dropna().tolist()\n",
        "        non_valid = [p for p in volcano_name if p not in names]\n",
        "        if non_valid:\n",
        "            raise ValueError(f\"Non valid names: {', '.join(non_valid)}\")\n",
        "    case 3:\n",
        "        df_Alaska=df[df['Region']==volcano_region]\n",
        "        volcano_name = df_Alaska['Volcano_Name'].dropna().tolist()\n"
      ],
      "metadata": {
        "id": "R2IMICGNW-NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if volcano_selection_method==1:\n",
        "  volcano_name=[dropdown.value]\n",
        "print(volcano_name)"
      ],
      "metadata": {
        "id": "2cvgl441boX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziQFIt-9t9yk"
      },
      "source": [
        "#*Main Code*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb4OyZi2N4z6"
      },
      "outputs": [],
      "source": [
        "for i in range(len(volcano_name)):\n",
        "  df2=df.loc[df['Volcano_Name']==volcano_name[i]]\n",
        "  images_dir = os.path.join(path, volcano_name[i])\n",
        "  os.makedirs(images_dir, exist_ok=True)\n",
        "  print(f\"-------------------------------\")\n",
        "  print(volcano_name[i])\n",
        "  print(f\"-------------------------------\")\n",
        "  latitude=df2.iloc[0]['Latitude']\n",
        "  longitude=df2.iloc[0]['Longitude']\n",
        "  volcano = ee.Geometry.Point([longitude,latitude])\n",
        "  study_zone = volcano.buffer(radius)\n",
        "  startDate = ee.Date(start_day);\n",
        "  endDate = ee.Date(end_day)\n",
        "  raw=ee.ImageCollection(\"COPERNICUS/S5P/NRTI/L3_SO2\").select('SO2_column_number_density','SO2_column_number_density_15km').filterBounds(study_zone).filterDate(startDate,endDate.advance(1, 'day'))\n",
        "  dateList = generateDateList(startDate, endDate);\n",
        "  One_image_per_day = ee.ImageCollection(dateList.map(One_image_per_dayfun)).map(CountBands)\n",
        "  res=1113.2\n",
        "  s5 = One_image_per_day \\\n",
        "      .filter(ee.Filter.eq('count', 2)) \\\n",
        "      .filter(ee.Filter.neq('Date', '23/03/2025')) \\\n",
        "      .map(InputPreparation)\n",
        "\n",
        "  timestamps = One_image_per_day.aggregate_array('system:time_start').getInfo()\n",
        "  date_strings = [datetime.utcfromtimestamp(ts / 1000).strftime('%Y-%m-%d') for ts in timestamps]\n",
        "\n",
        "  # Apply the model\n",
        "  ee_classifier = ee.Classifier.load(assetId)\n",
        "  classified = s5.map(lambda image: apply_classifier(image, ee_classifier))\n",
        "  ktonList = classified.aggregate_array('kton');\n",
        "\n",
        "  classified.sort('system:time_start')\n",
        "  #Lista di immagini\n",
        "  image_list = classified.toList(classified.size())\n",
        "\n",
        "  #Creazione file Excel\n",
        "  workbook = openpyxl.Workbook()\n",
        "  worksheet= workbook.active\n",
        "  worksheet.append(['Data', 'kton','kton_volcano'])\n",
        "\n",
        "  dates=[];\n",
        "  values=[];\n",
        "\n",
        "  #Estrazione valori in kton\n",
        "  for ii in range(image_list.size().getInfo()):\n",
        "    image = ee.Image(image_list.get(ii))\n",
        "    name=image.get('name').getInfo()\n",
        "    image2=image.addBands((image.select('classification').where(1,0)).paint(volcano.buffer(70000),1).rename('volcano'))\n",
        "    geemap.ee_export_image(image2.select('SO2corrected','masked_alt','classification','volcano'), filename=f\"{images_dir}/{name}.tif\", scale=res, region=study_zone, file_per_band=False)\n",
        "    raster = rs.open(f\"{images_dir}/{name}.tif\", \"r+\")\n",
        "    array = raster.read()\n",
        "    image_array=array[3]\n",
        "    im = array[0]\n",
        "    mask = array[2]\n",
        "    kernel2 = np.ones((60,60), np.uint8)\n",
        "    img_dilation = cv2.dilate(mask, kernel2, iterations=1)\n",
        "    labeled_mask = label(np.squeeze(img_dilation))\n",
        "    props = regionprops(labeled_mask)\n",
        "    tot=np.squeeze(mask+image_array)\n",
        "    kton2=0\n",
        "    so2_corrected2=np.zeros(np.shape(im))\n",
        "\n",
        "    for ii in range(len(props)+1):\n",
        "        if np.max(tot*(labeled_mask==ii))==2:\n",
        "          print('plume detected')\n",
        "          mask_volcano=(labeled_mask==ii)*(mask)\n",
        "          so2_corrected2=im*mask_volcano\n",
        "          kton2=np.nansum(so2_corrected2)\n",
        "          print(kton2)\n",
        "\n",
        "          \"\"\"plt.imshow((labeled_mask==ii)*(mask))\n",
        "          plt.savefig(f\"{images_dir}/{name}_mask.png\")\n",
        "          plt.show()\"\"\"\n",
        "\n",
        "          \"\"\"plt.imshow(so2_corrected2)\n",
        "          plt.savefig(f\"{images_dir}/{name}_so2.png\")\n",
        "          plt.show()\"\"\"\n",
        "\n",
        "    kton_value = image.get('kton').getInfo()\n",
        "    kton_value2 = kton2\n",
        "    date_value = image.get('Date').getInfo()\n",
        "    worksheet.append([name, kton_value, kton_value2])\n",
        "    print(f\"Data: {name}, kton_total: {kton_value}, kton_volcano: {kton_value2}\")\n",
        "    dates.append((datetime.strptime(date_value, \"%d/%m/%Y\")))\n",
        "    values.append((kton_value))\n",
        "    with rs.open(f\"{images_dir}/{name}.tif\", \"r+\") as update:\n",
        "      update.write(so2_corrected2,4)\n",
        "  #Salvataggio file\n",
        "  file_name = f\"{images_dir}/kton_dati.xlsx\"\n",
        "  workbook.save(file_name)\n",
        "\n",
        "  print(\"Dati salvati in:\", file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Map\n"
      ],
      "metadata": {
        "id": "7SuNfowAh7pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  filtered=classified.filterMetadata('kton','greater_than',0)\n",
        "  # Compute statistics (sum in this case)\n",
        "  stat=filtered.sum()\n",
        "\n",
        "  #geom = classified.first().geometry()\n",
        "  #bounds = geom.bounds().getInfo()['coordinates'][0]\n",
        "  box=ee.Geometry.Polygon([[[14.886997380371172, 5.824280032173915],[14.886997380371172, -6.93674077038431],[32.64090363037117, -6.93674077038431],[32.64090363037117, 5.824280032173915]]]);\n",
        "  vis_params2 = {\n",
        "    'min': 0,\n",
        "    'max': 0.005,\n",
        "    'palette': ['blue', 'purple', 'cyan', 'green', 'yellow', 'red'],\n",
        "    'opacity': 1,\n",
        "  };\n",
        "\n",
        "  vis_params1 = {\n",
        "    'min': 0,\n",
        "    'max': (filtered.size().getInfo()),\n",
        "    'palette': ['blue', 'purple', 'cyan', 'green', 'yellow', 'red'],\n",
        "    'opacity': 1,\n",
        "  };\n",
        "\n",
        "  geemap.update_package()\n",
        "  Map = geemap.Map(center=(latitude,longitude), zoom=6)\n",
        "  Map.add_basemap(\"HYBRID\")\n",
        "  Map.add_layer(stat.select('classification').selfMask(), vis_params1, ' Occurrencies')\n",
        "  Map.add_layer(stat.select('SO2corrected'), vis_params2, ' Cumulative')\n",
        "  Map.addLayer(volcano, {'color': 'black'}, 'Pin')\n",
        "\n",
        "  Map"
      ],
      "metadata": {
        "id": "xEL1BqqGh2_T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KvFewfOQXzFE",
        "0PqWs3IQM5f6",
        "7e1s0xrf7ztR"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
